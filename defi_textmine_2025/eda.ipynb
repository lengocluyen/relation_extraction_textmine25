{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(\n",
    "     level=logging.INFO, \n",
    "     format= '[%(asctime)s|%(levelname)s|%(module)s.py:%(lineno)s] %(message)s',\n",
    "     datefmt='%H:%M:%S'\n",
    " )\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from defi_textmine_2025.data.problem_formulation import (    \n",
    "    TextToMultiLabelDataGenerator,\n",
    "    Mention2TypeDataGenerator,\n",
    "    EntityBracketTaggingDataGenerator\n",
    ")\n",
    "from defi_textmine_2025.data.utils import (\n",
    "    load_labeled_raw_data,\n",
    "    load_test_raw_data,\n",
    "    clean_raw_dataset,\n",
    "    print_value_types,\n",
    "    save_data,\n",
    "    convert_text_to_entity_spans\n",
    ")\n",
    "from defi_textmine_2025.data.utils import TARGET_COL, INPUT_COLS, INTERIM_DIR, EDA_DIR\n",
    "\n",
    "VALIDATION_RATE = 0.25\n",
    "BASE_CHECKPOINT = \"camembert/camembert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_CHECKPOINT)\n",
    "def count_tokens(text: str, entity_types: list=None) -> int:\n",
    "    if entity_types:\n",
    "        return len(tokenizer(\"->\".join(entity_types), text)[\"input_ids\"])\n",
    "    else:\n",
    "        return len(tokenizer(text)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_df = load_labeled_raw_data()\n",
    "labeled_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_df = load_test_raw_data()\n",
    "test_raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check value types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of values in train data\n",
    "print_value_types(labeled_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of values in test data\n",
    "print_value_types(test_raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_df[labeled_raw_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_df[test_raw_df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_df.relations.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning/Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix value typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_clean_df = clean_raw_dataset(labeled_raw_df)\n",
    "test_clean_df = clean_raw_dataset(test_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (labeled_raw_df.index == labeled_clean_df.index).all()\n",
    "assert (test_raw_df.index == test_clean_df.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check value types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of values in train data\n",
    "print_value_types(labeled_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of values in test data\n",
    "print_value_types(test_clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned datasets\n",
    "\n",
    "For manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(labeled_clean_df, os.path.join(INTERIM_DIR, \"train_cleaned.csv\"))\n",
    "save_data(test_clean_df, os.path.join(INTERIM_DIR, \"test_cleaned.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All relation types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_classes = set(\n",
    "    sum(\n",
    "        labeled_clean_df.relations.apply(\n",
    "            lambda row: list({r for (e1, r, e2) in row})\n",
    "        ).values.tolist(),\n",
    "        [],\n",
    "    )\n",
    ")\n",
    "print(len(relation_classes))\n",
    "relation_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary stats on relation classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of text containing a relation category\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "n_texts_per_relation_type = pd.Series({cat: labeled_raw_df[labeled_raw_df.relations.str.contains(cat)].shape[0] for cat in relation_classes}).sort_values(ascending=True)\n",
    "n_texts_per_relation_type.plot.barh(xlabel=\"Number of texts containing the relation\", ylabel=\"Relation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of relation instances per text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rel_occur_per_text_df = pd.DataFrame({cat: labeled_raw_df.relations.str.count(cat) for cat in relation_classes})\n",
    "nb_rel_occur_per_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of the occurences of each category among texts\n",
    "nb_rel_occur_per_text_df.describe().T.sort_values(\"50%\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of instances per relation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rel_occur_per_text_df.sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_df.relations.str.count(\"IS_LOCATED_IN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_clean_df.relations.loc[181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total occurences of each category in the labeled dataset\n",
    "pd.Series({cat: labeled_raw_df.relations.str.count(cat).sum() for cat in relation_classes}).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "pd.Series({cat: labeled_raw_df.relations.str.count(cat).sum() for cat in relation_classes}).sort_values(ascending=True).plot.barh(xlabel=\"Number of labeled relations of a given category\", ylabel=\"Category of relation\", title=\"Total occurences of each category in the labeled dataset\")\n",
    "# pd.Series({cat: labeled_raw_df.relations.str.count(cat).sum() for cat in relation_classes}).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many relations can we expect to extract from a text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_relations_per_text_df = labeled_clean_df.relations.map(lambda x: len(x))\n",
    "print(n_relations_per_text_df.describe())\n",
    "n_relations_per_text_df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All entity types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_classes = set(\n",
    "    sum(\n",
    "        labeled_clean_df.entities.apply(\n",
    "            lambda row: list({e[\"type\"] for e in row})\n",
    "        ).values.tolist(),\n",
    "        [],\n",
    "    )\n",
    ")\n",
    "print(len(entity_classes))\n",
    "entity_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary stats on entity classes\n",
    "\n",
    "* entity type frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_raw_df[labeled_raw_df.entities.str.contains(\"MATERIEL\")].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "n_texts_per_entity_type = pd.Series({cat: labeled_raw_df[labeled_raw_df.entities.str.contains(cat)].shape[0] for cat in entity_classes}).sort_values(ascending=True)\n",
    "n_texts_per_entity_type.plot.barh(xlabel=\"Number of texts containing the entity\", ylabel=\"Entity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats about the cooccurence between entity categories and relation categories\n",
    "\n",
    "For each relation category:\n",
    "- what is the number of time each entity category is e1?\n",
    "- what is the number of time each entity category is e2?\n",
    "- what pairs of entity categories are never into a relation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_clean_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_idx, text, text_entities, text_relations in labeled_clean_df.reset_index().values:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entity_relation_cat_df  = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame([[text_idx, e1_id, e2_id, text_entities[e1_id][\"type\"], r_cat, text_entities[e2_id][\"type\"]] for e1_id, r_cat, e2_id in text_relations], columns=[\"text_id\", \"e1_id\", \"e2_id\", \"e1_cat\", \"r_cat\", \"e2_cat\"])\n",
    "        for text_idx, text, text_entities, text_relations in labeled_clean_df.reset_index().values\n",
    "    ],\n",
    "    axis=0\n",
    ")\n",
    "logging.info(f\"{entity_relation_cat_df.shape=}\")\n",
    "entity_relation_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(relation_classes), relation_classes)\n",
    "print(len(entity_classes), entity_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"r_cat=='HAS_CONTROL_OVER'\").e1_cat.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"r_cat=='HAS_CONTROL_OVER'\").e2_cat.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a relation, how many times is an entity category e1 or e2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15, 10))\n",
    "df = pd.concat([entity_relation_cat_df.value_counts(normalize=False), entity_relation_cat_df.value_counts(normalize=True)], axis=1).reset_index(drop=False)\n",
    "n_relation_classes = len(relation_classes)\n",
    "n_entity_classes = len(entity_classes)\n",
    "logging.info(f\"{df.shape[0]} existing relations vs. {n_entity_classes * n_relation_classes * n_entity_classes=} imaginable relations !\")\n",
    "df.to_csv(os.path.join(EDA_DIR, \"e1_cat-r_cat-e2_cat-freq.csv\")) #.plot.barh()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"r_cat=='STARTED_IN'\").drop(\"r_cat\", axis=1).value_counts().rename(\"count_STARTED_IN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"r_cat=='WAS_CREATED_IN'\").drop(\"r_cat\", axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e2_cat=='TIME_EXACT'\").drop([\"e1_cat\", \"e2_cat\", \"e1_id\", \"e2_id\"], axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e2_cat=='TIME_FUZZY'\").drop([\"e1_cat\", \"e2_cat\", \"e1_id\", \"e2_id\"], axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of relation categories between a pair of entity categories\n",
    "entity_relation_cat_df.drop([\"e1_id\", \"e2_id\"], axis=1).drop_duplicates().groupby([\"e1_cat\", \"e2_cat\"]).count().sort_values(by=\"r_cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what pairs of entity categories are in any relationship in the train dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_cat_pair_in_relation_df = entity_relation_cat_df[[\"e1_cat\", \"e2_cat\", \"e1_id\", \"e2_id\"]]#.drop_duplicates(subset=[\"e1_cat\", \"e2_cat\"])\n",
    "entity_cat_pair_in_relation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([entity_cat_pair_in_relation_df.value_counts(normalize=False), entity_cat_pair_in_relation_df.value_counts(normalize=True)], axis=1).reset_index(drop=False)\n",
    "n_entity_classes = len(entity_classes)\n",
    "logging.info(f\"{df.shape[0]} existing entity category pairs in relation vs. {n_entity_classes * n_entity_classes=} imaginable entity category pairs !\")\n",
    "df.to_csv(os.path.join(EDA_DIR, \"e1_cat-e2_cat-freq.csv\")) #.plot.barh()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many times the relation involves 2 entities of the same type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_cat_equal_e2_cat_df = entity_cat_pair_in_relation_df.query(\"e1_cat==e2_cat\").drop([\"e1_id\", \"e2_id\"], axis=1)\n",
    "e1_cat_equal_e2_cat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([e1_cat_equal_e2_cat_df.value_counts(normalize=False), e1_cat_equal_e2_cat_df.value_counts(normalize=True)], axis=1).reset_index(drop=False)\n",
    "n_entity_classes = len(entity_classes)\n",
    "logging.info(f\"{df.shape[0]} existing entity of identical categories in relation vs. {n_entity_classes=} imaginable entity category !\")\n",
    "df.to_csv(os.path.join(EDA_DIR, \"e1_cat-equal-e2_cat-freq.csv\")) #.plot.barh()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_r_cat_e1_cat_equal_e2_cat_df = entity_relation_cat_df.query(\"e1_cat==e2_cat\").drop([\"e1_id\", \"e2_id\"], axis=1)\n",
    "per_r_cat_e1_cat_equal_e2_cat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([per_r_cat_e1_cat_equal_e2_cat_df.value_counts(normalize=False), per_r_cat_e1_cat_equal_e2_cat_df.value_counts(normalize=True)], axis=1).reset_index(drop=False)\n",
    "n_entity_classes = len(entity_classes)\n",
    "logging.info(f\"{df.shape[0]} existing entity of identical categories in relation vs. {n_entity_classes=} imaginable entity category !\")\n",
    "df.to_csv(os.path.join(EDA_DIR, \"per_r_cat-e1_cat-equal-e2_cat_df-freq.csv\")) #.plot.barh()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what pairs of entity categories never have any relationship?\n",
    "\n",
    "- we don't need to attempt to classify pairs of entities of these types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_possible_entity_cat_pairs_df = pd.DataFrame([[e1_cat, e2_cat] for (e1_cat, e2_cat) in itertools.product(*[list(entity_classes)]*2)], columns=[\"e1_cat\", \"e2_cat\"]).set_index([\"e1_cat\", \"e2_cat\"])\n",
    "all_possible_entity_cat_pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_cat_pair_in_relation_df.drop_duplicates().set_index([\"e1_cat\", \"e2_cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binary relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type_pairs_in_binary_relation = set(entity_cat_pair_in_relation_df.query(\"e1_id != e2_id\")[[\"e1_cat\", \"e2_cat\"]].set_index([\"e1_cat\", \"e2_cat\"]).index.to_list())\n",
    "print(entity_type_pairs_in_binary_relation)\n",
    "len(entity_type_pairs_in_binary_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(all_possible_entity_cat_pairs_df.index.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(all_possible_entity_cat_pairs_df.index.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs_never_in_relation_df = pd.DataFrame(index=all_possible_entity_cat_pairs_df.index.difference(entity_cat_pair_in_relation_df.drop_duplicates().set_index([\"e1_cat\", \"e2_cat\"]).index)).reset_index(drop=False)\n",
    "entity_pairs_never_in_relation_df.to_csv(os.path.join(EDA_DIR, \"entity_pairs_never_in_relation.csv\"))\n",
    "entity_pairs_never_in_relation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unary relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_cat_pair_in_relation_df.query(\"e1_id == e2_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_types_in_unary_relation = set(entity_cat_pair_in_relation_df.query(\"e1_id == e2_id\")[[\"e1_cat\"]].set_index([\"e1_cat\"]).index.to_list())\n",
    "print(entity_types_in_unary_relation)\n",
    "len(entity_types_in_unary_relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e1_id == e2_id & e1_cat== 'PLACE'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e1_id == e2_id & e1_cat== 'ACCIDENT'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e1_id == e2_id & e1_cat== 'CIVILIAN'\").r_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e1_id == e2_id & e1_cat== 'TERRORIST_OR_CRIMINAL'\").r_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e1_id == e2_id & e1_cat== 'NON_MILITARY_GOVERNMENT_ORGANISATION'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e1_id == e2_id & e1_cat== 'GROUP_OF_INDIVIDUALS'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e1_id == e2_id & e1_cat== 'MILITARY'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the entity types involved in minority relation types\n",
    "\n",
    "... for data augmentation by entity replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, text_entities, text_relations = labeled_clean_df.iloc[100].to_list()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_text_to_entity_spans(text, text_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "n_texts_per_entity_type.plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e2_cat=='COLOR'\")#.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e1_cat=='MATERIEL'\").shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"r_cat=='HAS_CONSEQUENCE'\").shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"r_cat=='GENDER_FEMALE'\")#.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_texts_per_relation_type.rename(\"n_examples\").to_frame().query(\"n_examples < 500\").index.shape[0])\n",
    "n_texts_per_relation_type.rename(\"n_examples\").to_frame().query(\"n_examples < 500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_texts_per_relation_type.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(f\"r_cat=='IS_IN_CONTACT_WITH'\")#.text_id.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_relation_cat_df.query(\"e1_cat == e2_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_clean_df.loc[2455].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_clean_df[labeled_raw_df.relations.str.contains(\"HAS_LATITUDE\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        set_name: df.text.apply(len).describe()\n",
    "        for set_name, df in zip(\n",
    "            [\"labeled\", \"test\"],\n",
    "            [labeled_clean_df, test_clean_df],\n",
    "        )\n",
    "    }\n",
    ")#.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of tokens (subwords from CamemBERT tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        set_name: df.text.apply(count_tokens).describe()\n",
    "        for set_name, df in zip(\n",
    "            [\"labeled\", \"test\"],\n",
    "            [labeled_clean_df, test_clean_df],\n",
    "        )\n",
    "    }\n",
    ")#.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate interim datasets\n",
    "\n",
    "for each text, generate a csv file containing all the generated texts annotated as a multilabeled Text classification task\n",
    "\n",
    "The csv file is named after the index of the text in the raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_entity_pairs = entity_pairs_never_in_relation_df.set_index(['e1_cat', 'e2_cat']).index.to_list()\n",
    "excluded_entity_pairs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs_never_in_relation_df.query(\"e1_cat == e2_cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: tag entity role and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data_dir_path = os.path.join(INTERIM_DIR, \"entity_role_n_type_tagged_text_dataset\")\n",
    "# assert not os.path.exists(\n",
    "#     generated_data_dir_path\n",
    "# ), f\"You must delete this folder first {generated_data_dir_path}!\"\n",
    "\n",
    "data_generator = TextToMultiLabelDataGenerator(\n",
    "    allowed_binary_relation_entity_type_pairs=entity_type_pairs_in_binary_relation,\n",
    "    allowed_unary_relation_entity_types=entity_types_in_unary_relation\n",
    ")\n",
    "\n",
    "for split_name, clean_df in zip(\n",
    "    [\"test\", \"train\"],\n",
    "    [test_clean_df, labeled_clean_df],\n",
    "):\n",
    "    dest_dir_path = os.path.join(generated_data_dir_path, split_name)\n",
    "    for multilabel_data in (\n",
    "        pb := tqdm(\n",
    "            # data_generator.generate_row_multilabel_data(clean_df, only_w_relation=True if split_name!=\"test\" else False),\n",
    "            data_generator.generate_row_multilabel_data(clean_df, only_w_relation=False),\n",
    "            total=clean_df.shape[0],\n",
    "            desc=f\"{dest_dir_path} <- \",\n",
    "        )\n",
    "    ):\n",
    "        text_index = multilabel_data.iloc[0][data_generator.text_index_col]\n",
    "        dest_csv_file = os.path.join(dest_dir_path, f\"{text_index}.csv\")\n",
    "        pb.set_description(f\"{dest_csv_file} <-\")\n",
    "        save_data(multilabel_data, dest_csv_file, False)\n",
    "        break\n",
    "    # break\n",
    "multilabel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_data.iloc[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method2: Replace entity by entity type and role in a single tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data_dir_path = os.path.join(INTERIM_DIR, \"entity_mention2type_tagged_text_dataset\")\n",
    "# assert not os.path.exists(\n",
    "#     generated_data_dir_path\n",
    "# ), f\"You must delete this folder first {generated_data_dir_path}!\"\n",
    "\n",
    "data_generator = Mention2TypeDataGenerator(\n",
    "    allowed_binary_relation_entity_type_pairs=entity_type_pairs_in_binary_relation,\n",
    "    allowed_unary_relation_entity_types=entity_types_in_unary_relation\n",
    ")\n",
    "\n",
    "for split_name, clean_df in zip(\n",
    "    [\"test\", \"train\"],\n",
    "    [test_clean_df, labeled_clean_df],\n",
    "):\n",
    "    dest_dir_path = os.path.join(generated_data_dir_path, split_name)\n",
    "    for multilabel_data in (\n",
    "        pb := tqdm(\n",
    "            # data_generator.generate_row_multilabel_data(clean_df, only_w_relation=True if split_name!=\"test\" else False),\n",
    "            data_generator.generate_row_multilabel_data(clean_df, only_w_relation=False),\n",
    "            total=clean_df.shape[0],\n",
    "            desc=f\"{dest_dir_path} <- \",\n",
    "        )\n",
    "    ):\n",
    "        text_index = multilabel_data.iloc[0][data_generator.text_index_col]\n",
    "        dest_csv_file = os.path.join(dest_dir_path, f\"{text_index}.csv\")\n",
    "        pb.set_description(f\"{dest_csv_file} <-\")\n",
    "        save_data(multilabel_data, dest_csv_file, False)\n",
    "        break\n",
    "    # break\n",
    "multilabel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_data.loc[1][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methode3: Tagging entity mentions with brackets only to express their role in the relation\n",
    "\n",
    "- `{...}`: for the first entity\n",
    "- `[...]`: for the second entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data_dir_path = os.path.join(INTERIM_DIR, \"entity_bracket_tagging_dataset\")\n",
    "# assert not os.path.exists(\n",
    "#     generated_data_dir_path\n",
    "# ), f\"You must delete this folder first {generated_data_dir_path}!\"\n",
    "\n",
    "data_generator = EntityBracketTaggingDataGenerator(\n",
    "    allowed_binary_relation_entity_type_pairs=entity_type_pairs_in_binary_relation,\n",
    "    allowed_unary_relation_entity_types=entity_types_in_unary_relation\n",
    ")\n",
    "\n",
    "for split_name, clean_df in zip(\n",
    "    [\"test\", \"train\"],\n",
    "    [test_clean_df, labeled_clean_df],\n",
    "):\n",
    "    dest_dir_path = os.path.join(generated_data_dir_path, split_name)\n",
    "    for multilabel_data in (\n",
    "        pb := tqdm(\n",
    "            # data_generator.generate_row_multilabel_data(clean_df, only_w_relation=True if split_name!=\"test\" else False),\n",
    "            data_generator.generate_row_multilabel_data(clean_df, only_w_relation=False),\n",
    "            total=clean_df.shape[0],\n",
    "            desc=f\"{dest_dir_path} <- \",\n",
    "        )\n",
    "    ):\n",
    "        text_index = multilabel_data.iloc[0][data_generator.text_index_col]\n",
    "        dest_csv_file = os.path.join(dest_dir_path, f\"{text_index}.csv\")\n",
    "        pb.set_description(f\"{dest_csv_file} <-\")\n",
    "        save_data(multilabel_data, dest_csv_file, False)\n",
    "        # break\n",
    "    # break\n",
    "multilabel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_data.query(\"e1_id == e2_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tagged text size using a  data loader to load data from csv files\n",
    "\n",
    "to know wether the will fit at the input of the model (i.e. max of 512 tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add special tokens to the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define special tokens to add to the tokenizer\n",
    "# task_special_tokens = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"] + [\n",
    "#     f\"<{entity_class}>\" for entity_class in entity_classes\n",
    "# ]\n",
    "\n",
    "task_special_tokens = [f\"<{entity_class}>\" for entity_class in entity_classes]  # to specify the type of the 1st and 2nd entity\n",
    "print(task_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add special tokens to the tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(task_special_tokens, special_tokens=True)\n",
    "num_added_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    name=\"multilabel_tagged_text_dataset\",\n",
    "    data_dir=\"data/defi-text-mine-2025/interim/entity_bracket_tagging_dataset\",\n",
    "    streaming=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_dataset[\"test\"][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token_in_dataset_element(example: Dict[str, Any]) -> Dict[str, int]:\n",
    "    return {\"n_tokens\": count_tokens(example[\"text\"], [example[\"e1_type\"], example[\"e2_type\"]] if example[\"e2_id\"] != example[\"e2_id\"] else [example[\"e1_type\"]])}\n",
    "\n",
    "\n",
    "interim_dataset = interim_dataset.map(count_token_in_dataset_element)\n",
    "interim_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2ntokens_df = pd.DataFrame(\n",
    "    {\n",
    "        split_name: pd.Series(\n",
    "            [e[\"n_tokens\"] for e in tqdm(interim_dataset[split_name], split_name)],\n",
    "            name=f\"{split_name}_text_n_tokens\",\n",
    "        )\n",
    "        for split_name in interim_dataset.keys()\n",
    "    }\n",
    ")\n",
    "split2ntokens_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2ntokens_df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_dataset[\"test\"].filter(lambda x: x['n_tokens'] < 70)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only sentences of interes: mentioning the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `stanza` to split text into sentences\n",
    "\n",
    "With `re` to filter sentences of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interim_dataset[\"train\"].filter(lambda x: x[\"text_index\"] == 1175 and x[\"relations\"] == ['IS_LOCATED_IN'])\n",
    "interim_dataset[\"train\"].filter(lambda x: x[\"text_index\"] == 1175 and x[\"e1_id\"] == x['e2_id'] and x[\"relations\"] == \"['IS_LOCATED_IN']\")[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "from stanza import DownloadMethod\n",
    "import re\n",
    "\n",
    "lang = \"fr\"\n",
    "\n",
    "# text = \"Treize personnes ont trouvé la mort le 28 juin au matin dans le < sud du Togo >. Le minibus dans lequel ils se trouvaient a heurté un arbre, suite à l'éclatement de l'un de ses pneus sous l'effet d'une vitesse excessive. Selon un communiqué du ministre de la Sécurité, M. Billel Alibert, le bus qui transportait principalement des commerçants a dérapé à la suite de l'éclatement du pneu avant-droit. Le minibus s'est ensuite retrouvé sous le pont de la rivière, là où les corps ont été découverts. Dans sa chute, le bus a heurté un teck de 6 mètres de hauteur avant de se renverser sur son flanc droit, provoquant d'importants dégâts matériels et humains. L'accident s'est produit sur la nationale 1, reliant le < Togo > au Burkina Faso.\"\n",
    "# text = \"\"\"La 12e édition de la Journée de la Street Food s'est tenue ce [ 17 octobre 2015 ] à Londres. Malrgé la pluie, les { amateurs } de bonne cuisine étaient présents.\"\"\"\n",
    "text = \"\"\"Des milliers de personnes se sont retrouvées sur la Place de Cybèle pour exprimer leur soutien aux familles touchées par le drame du 06 mai 2015. En effet, près de 50 personnes ont perdu la vie dans l’explosion d’une centrale nucléaire. Les <victimes> étaient pour la plupart des <travailleurs> de la centrale. Lili-May Lopez, veuve d’un des ingénieurs, a créé l’association Justice pour nos Défunts pour réclamer que la lumière soit faite sur ce qui s’est réellement passé. Le Syndicat des Travailleurs Libres s’est joint à cette association afin de demander en plus, des conditions de travail plus sécurisées dans les centrales nucléaires du pays. Selon le responsable de ce syndicat, l’uranium n’est pas stocké selon les normes et les générateurs de vapeur et mobiliers sont vétustes. On retrouve même des rats dans les bâtiments qui rongent les câbles et documents importants. Le gouvernement a assuré aux manifestants que leurs doléances seront prises en compte.\"\"\"\n",
    "\n",
    "nlp = stanza.Pipeline(lang=lang, processors='tokenize', download_method=DownloadMethod.REUSE_RESOURCES)\n",
    "e1_pattern = re.compile(\".*[\\{\\}<>].*\")\n",
    "e2_pattern = re.compile(\".*[\\[\\]].*\")\n",
    "final_text_sentences = []\n",
    "doc = nlp(text)\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    # print(f'====== Sentence {i+1} tokens =======')\n",
    "    sentence_text = \" \".join([token.text for token in sentence.tokens])\n",
    "    if e1_pattern.match(sentence_text) and e2_pattern.match(sentence_text):\n",
    "        final_text_sentences = [sentence_text]\n",
    "        break\n",
    "    if e1_pattern.match(sentence_text) or e2_pattern.match(sentence_text):\n",
    "        final_text_sentences.append(sentence_text)\n",
    "\" \".join(final_text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 16:10:51 WARNING: Language fr package default expects mwt, which has been added\n",
      "2024-10-06 16:10:51 INFO: Loading these models for language: fr (French):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2024-10-06 16:10:51 INFO: Using device: cuda\n",
      "2024-10-06 16:10:51 INFO: Loading: tokenize\n",
      "2024-10-06 16:10:52 INFO: Loading: mwt\n",
      "2024-10-06 16:10:52 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Des milliers de personnes se sont retrouvées sur la Place de Cybèle pour exprimer leur soutien aux familles touchées par le drame du 06 mai 2015.\n",
      "En effet, près de 50 personnes ont perdu la vie dans l’explosion d’une centrale nucléaire.\n",
      "Les <victimes> étaient pour la plupart des <travailleurs> de la centrale.\n",
      "Lili-May Lopez, veuve d’un des ingénieurs, a créé l’association Justice pour nos Défunts pour réclamer que la lumière soit faite sur ce qui s’est réellement passé.\n",
      "Le Syndicat des Travailleurs Libres s’est joint à cette association afin de demander en plus, des conditions de travail plus sécurisées dans les centrales nucléaires du pays.\n",
      "Selon le responsable de ce syndicat, l’uranium n’est pas stocké selon les normes et les générateurs de vapeur et mobiliers sont vétustes.\n",
      "On retrouve même des rats dans les bâtiments qui rongent les câbles et documents importants.\n",
      "Le gouvernement a assuré aux manifestants que leurs doléances seront prises en compte.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from stanza import DownloadMethod\n",
    "import re\n",
    "\n",
    "lang = \"fr\"\n",
    "\n",
    "text = \"\"\"Des milliers de personnes se sont retrouvées sur la Place de Cybèle pour exprimer leur soutien aux familles touchées par le drame du 06 mai 2015. En effet, près de 50 personnes ont perdu la vie dans l’explosion d’une centrale nucléaire. Les <victimes> étaient pour la plupart des <travailleurs> de la centrale. Lili-May Lopez, veuve d’un des ingénieurs, a créé l’association Justice pour nos Défunts pour réclamer que la lumière soit faite sur ce qui s’est réellement passé. Le Syndicat des Travailleurs Libres s’est joint à cette association afin de demander en plus, des conditions de travail plus sécurisées dans les centrales nucléaires du pays. Selon le responsable de ce syndicat, l’uranium n’est pas stocké selon les normes et les générateurs de vapeur et mobiliers sont vétustes. On retrouve même des rats dans les bâtiments qui rongent les câbles et documents importants. Le gouvernement a assuré aux manifestants que leurs doléances seront prises en compte.\"\"\"\n",
    "\n",
    "nlp = stanza.Pipeline(lang=lang, processors='tokenize', download_method=DownloadMethod.REUSE_RESOURCES)\n",
    "e1_pattern = re.compile(\".*[\\{\\}<>].*\")\n",
    "e2_pattern = re.compile(\".*[\\[\\]].*\")\n",
    "final_text_sentences = []\n",
    "doc = nlp(text)\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(sentence.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply over the whole datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import stanza\n",
    "from stanza import DownloadMethod\n",
    "import re\n",
    "\n",
    "lang = \"fr\"\n",
    "\n",
    "nlp = stanza.Pipeline(lang=lang, processors='tokenize', download_method=DownloadMethod.REUSE_RESOURCES)\n",
    "e1_pattern = re.compile(\".*[\\{\\}<>].*\")\n",
    "e2_pattern = re.compile(\".*[\\[\\]].*\")\n",
    "\n",
    "def reduce_a_text_to_text_of_interest(tagged_text: str) -> str:\n",
    "    final_text_sentences = []\n",
    "    doc = nlp(text)\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        # print(f'====== Sentence {i+1} tokens =======')\n",
    "        # sentence_text = \" \".join([token.text for token in sentence.tokens])\n",
    "        if e1_pattern.match(sentence.text) and e2_pattern.match(sentence.text):\n",
    "            final_text_sentences = [sentence_text]\n",
    "            break\n",
    "        if e1_pattern.match(sentence.text) or e2_pattern.match(sentence.text):\n",
    "            final_text_sentences.append(sentence.text)\n",
    "    return \" \".join(final_text_sentences)\n",
    "\n",
    "def reduce_texts_to_text_of_interest(examples) -> List[Dict]:\n",
    "    return examples | {'text_of_interest': [reduce_a_text_to_text_of_interest(text) for text in examples['text']]}\n",
    "\n",
    "interim_dataset[\"train\"] = interim_dataset[\"train\"].map(reduce_texts_to_text_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgl11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
